{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Copyright April 1, 2018 Warren E. Agin\n",
    "#This code is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "#You may obtain a copy of the license at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode\n",
    "\n",
    "#This code is derived from sample code available at https://www.tensorflow.org/tutorials/wide,\n",
    "#licensed under the Apache License, Version 2.0. The license notice for the original\n",
    "#code is provided below.\n",
    "\n",
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "#THE FILES HAVE BEEN CHANGED FROM THE VERSION RELEASED BY THE TENSORFLOW AUTHORS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to build and run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_URL = ''\n",
    "TRAINING_FILE = 'trainingFile.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "EVAL_FILE = 'testFile.csv'\n",
    "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create feature column names and default types using featureNames.csv file\n",
    "\n",
    "with open('featureNames.csv', 'r', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        _CSV_COLUMNS = row\n",
    "print(_CSV_COLUMNS)        \n",
    "#dtypes for each field, needs to be defined manually        \n",
    "_CSV_COLUMN_DEFAULTS=[[0],[0],[0],[0],[0],[0.0],[0],[0],[0],[0],[0],[0.0],[0.0],[0.0],[0],[0.0],[0.0],[0.0],[0],[0.0],[0.0],[0.0],[0],[0],[0.0],[0.0],[0.0],[0],[0.0],[0.0],[0.0],[0.0],[0.0]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up variables for use with the model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model_dir', type=str, default='/Users/trustee/Documents/Transport/Temp for 13 project/model_data',\n",
    "    help='Base directory for the model.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model_type', type=str, default='deep',\n",
    "    help=\"Valid model types: {'wide', 'deep', 'wide_deep'}.\")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_epochs', type=int, default=50, help='Number of training epochs.') \n",
    "\n",
    "parser.add_argument(\n",
    "    '--epochs_per_eval', type=int, default=2,\n",
    "    help='The number of training epochs to run between evaluations.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batch_size', type=int, default=20, help='Number of examples per batch.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--train_data', type=str, default='trainingFile.csv',  #revise to match your location and directory structure\n",
    "    help='Path to the training data.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--test_data', type=str, default='testFile.csv',  #revise to match your location and directory structure\n",
    "    help='Path to the test data.')\n",
    "\n",
    "_l1_strength=0.0\n",
    "#Strength of the l1 regularization for the wide model - 0 to 1. 0 is off\n",
    "\n",
    "_l2_strength=0.0\n",
    "#Strength of the l2 regularization for the wide model - 0 to 1. 0 is off\n",
    "\n",
    "#need to manually fill in dataset sizes. Information is in LEARNER_README.txt\n",
    "\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 400000,\n",
    "    'validation': 30000,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_columns():\n",
    "    \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
    "    # Continuous columns\n",
    "    NTRDBT=tf.feature_column.categorical_column_with_identity('NTRDBT', num_buckets=2)\n",
    "    JOINT=tf.feature_column.categorical_column_with_identity('JOINT', num_buckets=2)\n",
    "    ORGD1FPRSE=tf.feature_column.categorical_column_with_identity('ORGD1FPRSE', num_buckets=2)\n",
    "    PRFILE=tf.feature_column.categorical_column_with_identity('PRFILE', num_buckets=2)\n",
    "    DISTSUCCESS=tf.feature_column.numeric_column('DISTSUCCESS')\n",
    "    FEEP=tf.feature_column.categorical_column_with_identity('FEEP', num_buckets=2)\n",
    "    FEEI=tf.feature_column.categorical_column_with_identity('FEEI', num_buckets=2)\n",
    "    FEEW=tf.feature_column.categorical_column_with_identity('FEEW', num_buckets=2)\n",
    "    REALPROPNULL=tf.feature_column.categorical_column_with_identity('REALPROPNULL', num_buckets=2)\n",
    "    REALPROPNONE=tf.feature_column.categorical_column_with_identity('REALPROPNONE', num_buckets=2)\n",
    "    REALPROPVALUE=tf.feature_column.numeric_column('REALPROPVALUE')\n",
    "    REALPROPVALUESQR=tf.feature_column.numeric_column('REALPROPVALUESQR')\n",
    "    REALPROPVALUELOG=tf.feature_column.numeric_column('REALPROPVALUELOG')\n",
    "    PERSPROPNULL=tf.feature_column.categorical_column_with_identity('PERSPROPNULL', num_buckets=2)\n",
    "    PERSPROPVALUE=tf.feature_column.numeric_column('PERSPROPVALUE')\n",
    "    PERSPROPVALUESQR=tf.feature_column.numeric_column('PERSPROPVALUESQR')\n",
    "    PERSPROPVALUELOG=tf.feature_column.numeric_column('PERSPROPVALUELOG')\n",
    "    UNSECNPRNULL=tf.feature_column.categorical_column_with_identity('UNSECNPRNULL', num_buckets=2)\n",
    "    UNSECNPRVALUE=tf.feature_column.numeric_column('UNSECNPRVALUE')\n",
    "    UNSECNPRVALUESQR=tf.feature_column.numeric_column('UNSECNPRVALUESQR')\n",
    "    UNSECNPRVALUELOG=tf.feature_column.numeric_column('UNSECNPRVALUELOG')\n",
    "    UNSECEXCESS=tf.feature_column.categorical_column_with_identity('UNSECEXCESS', num_buckets=2)\n",
    "    UNSECPRNULL=tf.feature_column.categorical_column_with_identity('UNSECPRNULL', num_buckets=2)\n",
    "    UNSECPRVALUE=tf.feature_column.numeric_column('UNSECPRVALUE')\n",
    "    UNSECPRVALUESQR=tf.feature_column.numeric_column('UNSECPRVALUESQR')\n",
    "    UNSECPRVALUELOG=tf.feature_column.numeric_column('UNSECPRVALUELOG')\n",
    "    AVGMNTHINULL=tf.feature_column.categorical_column_with_identity('AVGMNTHINULL', num_buckets=2)\n",
    "    AVGMNTHIVALUE=tf.feature_column.numeric_column('AVGMNTHIVALUE')\n",
    "    AVGMNTHIVALUESQR=tf.feature_column.numeric_column('AVGMNTHIVALUESQR')\n",
    "    AVGMNTHIVALUELOG=tf.feature_column.numeric_column('AVGMNTHIVALUELOG')\n",
    "    IEINDEX=tf.feature_column.numeric_column('IEINDEX')\n",
    "    IEGAP=tf.feature_column.numeric_column('IEGAP')\n",
    "    \n",
    "    RealPropValueBuckets = tf.feature_column.bucketized_column(REALPROPVALUE, boundaries=[50000,100000,150000,200000,250000,300000,350000,400000,500000,600000])\n",
    "    PersPropValueBuckets = tf.feature_column.bucketized_column(PERSPROPVALUE, boundaries=[5000,10000,15000,20000,25000,30000,35000,40000,50000,60000,70000,80000,90000,100000])\n",
    "    UnsecBuckets = tf.feature_column.bucketized_column(UNSECNPRVALUE, boundaries=[5000,10000,15000,20000,25000,30000,35000,40000,50000,60000,70000,80000,90000,100000,140000])\n",
    "    PrioBuckets = tf.feature_column.bucketized_column(UNSECPRVALUE, boundaries=[5000,10000,15000,20000,25000,30000,35000,40000,50000,60000])\n",
    "    IncomeBuckets = tf.feature_column.bucketized_column(AVGMNTHIVALUE, boundaries=[100,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,10000,20000])\n",
    "    \n",
    "    # Wide columns and deep columns.  #this section is not needed to run the neural network\n",
    "    base_columns = [\n",
    "    #    NTRDBT,\n",
    "        JOINT,\n",
    "        ORGD1FPRSE,\n",
    "    #    PRFILE,\n",
    "        DISTSUCCESS,\n",
    "    #    FEEP,\n",
    "    #    FEEI,\n",
    "    #    FEEW,\n",
    "        REALPROPNULL,\n",
    "        REALPROPNONE,\n",
    "    #    REALPROPVALUE,\n",
    "    #    REALPROPVALUESQR,\n",
    "    #    REALPROPVALUELOG,\n",
    "    #    PERSPROPNULL,\n",
    "    #    PERSPROPVALUE,\n",
    "    #    PERSPROPVALUESQR,\n",
    "    #    PERSPROPVALUELOG,\n",
    "    #    UNSECNPRNULL,\n",
    "    #    UNSECNPRVALUE,\n",
    "    # #   UNSECNPRVALUESQR,\n",
    "    #    UNSECNPRVALUELOG,\n",
    "    #    UNSECEXCESS,\n",
    "    #    UNSECPRNULL,\n",
    "    #    UNSECPRVALUE,\n",
    "    #    UNSECPRVALUESQR,\n",
    "    #    UNSECPRVALUELOG,\n",
    "    #    AVGMNTHINULL,\n",
    "    #    AVGMNTHIVALUE,\n",
    "    #    AVGMNTHIVALUESQR,\n",
    "    #    AVGMNTHIVALUELOG,\n",
    "    #    IEINDEX,\n",
    "        IEGAP,\n",
    "        RealPropValueBuckets,\n",
    "      ]\n",
    "\n",
    "    crossed_columns = []   #no crossed columns in this implementation \n",
    "\n",
    "    wide_columns = base_columns + crossed_columns\n",
    "\n",
    "    deep_columns = [    #features not used in the neural network have been commented out\n",
    "        tf.feature_column.indicator_column(NTRDBT),\n",
    "        tf.feature_column.indicator_column(JOINT),\n",
    "        tf.feature_column.indicator_column(ORGD1FPRSE),\n",
    "        tf.feature_column.indicator_column(PRFILE),\n",
    "        DISTSUCCESS,\n",
    "        tf.feature_column.indicator_column(FEEP),\n",
    "        tf.feature_column.indicator_column(FEEI),\n",
    "        tf.feature_column.indicator_column(FEEW),\n",
    "        tf.feature_column.indicator_column(REALPROPNULL),\n",
    "        tf.feature_column.indicator_column(REALPROPNONE),\n",
    "   #     REALPROPVALUE,\n",
    "   #     REALPROPVALUESQR,\n",
    "   #     REALPROPVALUELOG,\n",
    "        tf.feature_column.indicator_column(PERSPROPNULL),\n",
    "   #     PERSPROPVALUE,\n",
    "   #     PERSPROPVALUESQR,\n",
    "   #     PERSPROPVALUELOG,\n",
    "        tf.feature_column.indicator_column(UNSECNPRNULL),\n",
    "   #     UNSECNPRVALUE,\n",
    "   #     UNSECNPRVALUESQR,\n",
    "   #     UNSECNPRVALUELOG,\n",
    "        tf.feature_column.indicator_column(UNSECEXCESS),\n",
    "        tf.feature_column.indicator_column(UNSECPRNULL),\n",
    "   #     UNSECPRVALUE,\n",
    "   #     UNSECPRVALUESQR,\n",
    "   #     UNSECPRVALUELOG,\n",
    "        tf.feature_column.indicator_column(AVGMNTHINULL),\n",
    "   #     AVGMNTHIVALUE,\n",
    "   #     AVGMNTHIVALUESQR,\n",
    "   #     AVGMNTHIVALUELOG,\n",
    "        IEINDEX,\n",
    "        IEGAP,\n",
    "        RealPropValueBuckets,\n",
    "        PersPropValueBuckets, \n",
    "        UnsecBuckets,\n",
    "        PrioBuckets,\n",
    "        IncomeBuckets,\n",
    "    ]\n",
    "\n",
    "    return wide_columns, deep_columns\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "    \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "    wide_columns, deep_columns = build_model_columns()\n",
    "    hidden_units = [256, 128, 64, 32]\n",
    "\n",
    "    # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "    # trains faster than GPU for this model.\n",
    "    run_config = tf.estimator.RunConfig().replace(\n",
    "      session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "    if model_type == 'wide':\n",
    "        return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=wide_columns, optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=_l1_strength,       \n",
    "        l2_regularization_strength=_l2_strength),\n",
    "        config=run_config)  \n",
    "    \n",
    "    elif model_type == 'deep':\n",
    "        return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "    \n",
    "    else:\n",
    "        return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "\n",
    "    def parse_csv(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('SUCCESS')            \n",
    "        return features, tf.equal(labels, 1)      \n",
    "\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])  \n",
    "\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set logging and parsing\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "trainAccuracyLog={}\n",
    "testAccuracyLog={}\n",
    "\n",
    "# Clean up the model directory if present\n",
    "shutil.rmtree(FLAGS.model_dir, ignore_errors=True)\n",
    "model = build_estimator(FLAGS.model_dir, FLAGS.model_type)\n",
    "\n",
    "# Train and evaluate the model every `FLAGS.epochs_per_eval` epochs.\n",
    "for n in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        FLAGS.train_data, FLAGS.epochs_per_eval, True, FLAGS.batch_size))\n",
    "    \n",
    "    train_results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        FLAGS.train_data, 1, False, FLAGS.batch_size))\n",
    "\n",
    "    test_results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        FLAGS.test_data, 1, False, FLAGS.batch_size))\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * FLAGS.epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "    \n",
    "    if n == 24:\n",
    "        for key in sorted(train_results):\n",
    "            print('Training: %s: %s' % (key, train_results[key]))\n",
    "\n",
    "    for key in sorted(test_results):\n",
    "        print('Test: %s: %s' % (key, test_results[key]))\n",
    "        \n",
    "    trainAccuracyLog[n + 1]=train_results['accuracy']\n",
    "    testAccuracyLog[n + 1]=test_results['accuracy']\n",
    "\n",
    "#write accuracy numbers to a log file for later review    \n",
    "with open(\"log.csv\", \"w\", newline='') as log:\n",
    "    w = csv.writer(log)\n",
    "    w.writerow(['epoch','training accuracy','testing accuracy'])\n",
    "    for key in trainAccuracyLog:\n",
    "        w.writerow([key, trainAccuracyLog[key], testAccuracyLog[key]])\n",
    "       \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
