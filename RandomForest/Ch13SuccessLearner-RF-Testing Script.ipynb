{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to build and run the final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Copyright April 1. 2018, Warren E. Agin\n",
    "#Code released under the Creative Commons Attribution-NonCommercial-\n",
    "#ShareAlike 4.0 International License. You may obtain a copy of the license at \n",
    "#https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_URL = ''\n",
    "FEATURE_NAMES = 'featureNames.csv'\n",
    "TRAINING_FILE = 'trainingFile.csv'\n",
    "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
    "\n",
    "#DELETED_FEATURES allows removal of selected features from the training set\n",
    "DELETED_FEATURES = [\n",
    "    'REALPROPVALUESQR',\n",
    "    'REALPROPVALUELOG',\n",
    "    'PERSPROPVALUESQR',\n",
    "    'PERSPROPVALUELOG',\n",
    "    'UNSECNPRVALUESQR',\n",
    "    'UNSECNPRVALUELOG',\n",
    "    'UNSECPRVALUESQR',\n",
    "    'UNSECPRVALUELOG',\n",
    "    'AVGMNTHIVALUESQR',\n",
    "    'AVGMNTHIVALUELOG',\n",
    "    'NTRDBT',\n",
    "#    'JOINT',\n",
    "#    'ORGD1FPRSE',\n",
    "#    'PRFILE',\n",
    "#    'DISTSUCCESS',\n",
    "    'FEEP',\n",
    "#    'FEEI',\n",
    "    'FEEW',\n",
    "    'REALPROPNULL',\n",
    "#    'REALPROPNONE',\n",
    "#    'REALPROPVALUE',\n",
    "    'PERSPROPNULL',\n",
    "#    'PERSPROPVALUE',\n",
    "#    'UNSECNPRNULL',\n",
    "#    'UNSECNPRVALUE',\n",
    "    'UNSECEXCESS',\n",
    "    'UNSECPRNULL',\n",
    "#    'UNSECPRVALUE',\n",
    "    'AVGMNTHINULL',\n",
    "#    'AVGMNTHIVALUE',\n",
    "#    'IEINDEX',\n",
    "#    'IEGAP'\n",
    "]\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#identify features from data sets\n",
    "features = pd.read_csv(FEATURE_NAMES)\n",
    "featureNames = list(features.columns)\n",
    "\n",
    "#read training set into panda arrays\n",
    "training = pd.read_csv(TRAINING_FILE, names=featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert the panda arrays to numpy arrays for use with the learner\n",
    "#creates training as a numpy array, trainingLabels as a numpy\n",
    "#array holding the success field, and featureNames as a list of the features\n",
    "\n",
    "def removeFeatures(file):\n",
    "    for each in DELETED_FEATURES:\n",
    "        file=file.drop(each, axis=1)\n",
    "    return(file)\n",
    "\n",
    "def convert2np(file):   \n",
    "    labels = np.array(file['SUCCESS'])   #copy out the success column as a numpy array\n",
    "    file = file.drop('SUCCESS', axis=1)  #remove the success column\n",
    "    file = removeFeatures(file)          #remove the features not being used\n",
    "    file = np.array(file)                #convert the data to a numpy array\n",
    "    return(file,labels)\n",
    "    \n",
    "training,trainingLabels = convert2np(training)\n",
    "\n",
    "finalFeatureNames = featureNames\n",
    "for each in DELETED_FEATURES:\n",
    "    finalFeatureNames.remove(each)\n",
    "finalFeatureNames.remove('SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create, run and evaluate the model\n",
    "\n",
    "#set logging file and add a list of the features being used\n",
    "log = 'Features used: ' + str(finalFeatureNames) + '\\r\\n'\n",
    "\n",
    "#function to calculate metrics - calculates accuracy, AUC and a confusion matrix\n",
    "def calcResults(set, labels):\n",
    "    # Use the predict method on the training and test data\n",
    "    predictions = rf.predict(set)\n",
    "    # Calculate the number of errors\n",
    "    numberErrors = sum(abs(predictions - labels))\n",
    "\n",
    "    # Calculate and display accuracy and other statistics\n",
    "    accuracy = (1-(numberErrors/len(set)))*100\n",
    "    aucResult = metrics.roc_auc_score(labels, predictions)\n",
    "    cMatrix = metrics.confusion_matrix(labels, predictions)\n",
    "\n",
    "    return(accuracy, aucResult, cMatrix)\n",
    "\n",
    "\n",
    "#define characteristics for the learner\n",
    "n_estimators = 1000\n",
    "max_features = 'auto' #default is 'auto' which considers sqrt(n_features) at each split - alt are 1 to consider n_features or a decimal\n",
    "max_depth = 20    #default is 'None'\n",
    "min_samples_split = 150 #default is 2\n",
    "min_samples_leaf = 1  #default is 1\n",
    "random_state = 26 #default is 'None' but use a number for testing variations\n",
    "\n",
    "log += ' n_estimators: %s \\r\\n max_features: %s \\r\\n max_depth: %s \\r\\n min_samples_split: %s \\r\\n min_samples_leaf: %s \\r\\n' % (n_estimators,max_features,max_depth,min_samples_split,min_samples_leaf)\n",
    "\n",
    "# Instantiate model with n_estimators decision trees\n",
    "rf = sk.ensemble.RandomForestClassifier(criterion = 'gini', n_estimators = n_estimators, max_features = max_features, max_depth = max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, random_state = random_state)\n",
    "\n",
    "# criterion = 'gini' or 'entropy'\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(training, trainingLabels);\n",
    "\n",
    "#run predictions on the training set and the test set and calculate metrics\n",
    "\n",
    "accuracy, aucResult, cMatrix = calcResults(training, trainingLabels)\n",
    "\n",
    "log += 'Train Set Accuracy: %s \\r\\n' %  round(accuracy, 2)\n",
    "log += 'Train set AUC: %s  \\r\\n' % round(aucResult, 4)\n",
    "\n",
    "# Print out the confusion matrix.\n",
    "print('accuracy: %s' % round(accuracy, 2))\n",
    "\n",
    "\n",
    "# Get numerical feature importances \n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 6)) for feature, importance in zip(finalFeatureNames, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Log the features and importances \n",
    "\n",
    "for pair in feature_importances:\n",
    "    log += 'Variable: {:20} Importance: {}\\r\\n'.format(*pair)\n",
    "log += '\\r\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run the model against the three test sets\n",
    "\n",
    "#define the sources for the test sets\n",
    "EVAL_FILE1 = 'test1File.csv'\n",
    "EVAL_FILE2 = 'test2File.csv'\n",
    "EVAL_FILE3 = 'test3File.csv'\n",
    "\n",
    "def evaluateModel(set):\n",
    "    \n",
    "    results=''\n",
    "    \n",
    "    #load the test file into panda array and convert into x and label numpy arrays\n",
    "    testing = pd.read_csv(set, names=featureNames)\n",
    "    testing,testingLabels = convert2np(testing)\n",
    "\n",
    "    #run the model on the test set and return accuracy, AUC, and confusion matrix\n",
    "    accuracy, aucResult, cMatrix = calcResults(testing, testingLabels)\n",
    "\n",
    "    #log the results\n",
    "    results += 'Test Set Accuracy: %s \\r\\n' %  round(accuracy, 2)\n",
    "    results += 'Test set AUC: %s  \\r\\n' % round(aucResult, 4)\n",
    "    results += str(cMatrix)\n",
    "    results += '\\r\\n\\r\\n'\n",
    "    print('Test Set Evaluated')\n",
    "    return(results)\n",
    "\n",
    "log += evaluateModel(EVAL_FILE1)\n",
    "log += evaluateModel(EVAL_FILE2)\n",
    "log += evaluateModel(EVAL_FILE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print the log file for review   \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the log file to LOG.txt\n",
    "with open('LOG.txt', 'w', newline='') as f:\n",
    "    f.write(log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for deployment\n",
    "\n",
    "filename = 'finalRFModel.sav'\n",
    "pickle.dump(rf, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run probabilities and log out for analysis in Excel\n",
    "\n",
    "#load the test file into panda array and convert into x and label numpy arrays\n",
    "testingProb = pd.read_csv(EVAL_FILE1, names=featureNames)\n",
    "testingProb,testingLabels = convert2np(testingProb)\n",
    "\n",
    "#compute the failure, success probabilities for each in testingProb\n",
    "probabilities = rf.predict_proba(testingProb)\n",
    "\n",
    "#write probabilities and actual result to a csv\n",
    "with open('ProbLog.csv', 'w', newline='') as f:\n",
    "    writer=csv.writer(f, delimiter=',')\n",
    "    for key in range(0,len(probabilities)):\n",
    "        writer.writerow([probabilities[key][0],probabilities[key][1],testingLabels[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample code for reloading the saved model\n",
    "testing = '' #name of the data file with data for prediction\n",
    "rfLoaded = pickle.load(open('finalRFModel.sav', 'rb'))\n",
    "rfLoaded.predict_proba(testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
